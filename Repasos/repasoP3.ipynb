{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b0530c",
   "metadata": {},
   "source": [
    "# Árboles de decisión\n",
    "\n",
    "Son modelos predictivos, que gracias a su estructura de niveles y nodos, se pueden separar los datos, de tal manera, que se compara si tu dato en cuestión sobrepasa el umbral establecido en una característica específica. \n",
    "\n",
    "## Regresión\n",
    "\n",
    "Primero el algoritmo utiliza todas las características y umbrales de división posibles, y con ello, seleccionar los umbrales que logren la mayor reducción de varianza.\n",
    "\n",
    "$$\n",
    "\\text{Reducción de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Esta formula compara la varianza total de las ramas con la varianza despues de ser ponderada por la cantidad de datos en cada rama.\n",
    "\n",
    "## Clasificación\n",
    "\n",
    "Igual que el de regresión, el algoritmo utiliza todas las características y umbrales de división posibles, y con ello, seleccionar que los umbrales que, ahora en este caso, logren la mayor reducción de impureza, calculada por medio de Gini o la Entropía.\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum p_i^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entropía = -\\sum p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Impureza} = \\text{Impureza Nodo Padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Esta formula compara la impureza del nodo padre con la impureza ponderada por la cantidad de datos en cada rama.\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "+ Fácilmente interpretables gracias a las reglas y umbrales establecidos en el árbol de decisión. \n",
    "+ No es necesario escalar los datos\n",
    "\n",
    "## Desventajas\n",
    "\n",
    "+ Estos modelos pueden tender al overfitting debido a que se puede llegar a crear un árbol tan complejo, que crezca tanto que memorice los datos.\n",
    "+ No tan buen poder predictivo.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c4821",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Utilizando el método de bootstrap, se realiza un muestreo aleatorio para crear varios árboles diferentes. Estos árboles son entrenados y para realizar las predicciones cambiará dependiendo de si es regresión o clasificación. \n",
    "\n",
    "+ Si es regresión se promediará la predicción de todos los árboles creados.\n",
    "+ Si es de clasificación, se toma la categoría más repetida.\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "+ Mayor poder predictivo de comparado a los arboles de decisión normales.\n",
    "+ Gracias al muestreo aleatorio se reduce en gran medida el overfitting.\n",
    "+ No es necesario escalamiento de datos.\n",
    "\n",
    "## Desventajas\n",
    "\n",
    "+ Pierde interpretabilidad.\n",
    "+ Es mas costoso computacionalmente.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e7e76",
   "metadata": {},
   "source": [
    "# Prueba de poder\n",
    "\n",
    "Es una técnica utilizada para determinar el tamaño de muestra necesario para lograr observar un efecto con una probabilidad determinada, así como tambie, para ver que tan probable es que mi estudio detecte un efecto real (si es que existe).\n",
    "\n",
    "## Error tipo I $\\alpha$\n",
    "\n",
    "Falso positivo: rechazamos $H_0$ cuando en realidad es verdadera\n",
    "\n",
    "## Error tipo II ($\\beta$)\n",
    "\n",
    "Falso negativo: No rechazamos $H_0$ cuando en realidad $H_A$ es la verdadera\n",
    "\n",
    "| Tipo de Error | Decisión Tomada  | Realidad         | Consecuencia |\n",
    "|--------------|-----------------|-----------------|--------------|\n",
    "| **Tipo I $\\alpha$** | Cambiamos a rojo | No había diferencia | Cambio innecesario, pérdida de recursos |\n",
    "| **Tipo II $\\beta$** | No cambiamos a rojo | Rojo era mejor | Perdemos una oportunidad de mejora |\n",
    "\n",
    "## Elementos clave\n",
    "- Potencia : (1 - $\\beta$) Es la probabilidad de rechazar $H_0$ cuando $H_A$ es negativa (Error tipo II). Usualmente queremos tener $\\beta$ en 80%\n",
    "- Nivel de significancia: $\\alpha$ Es la probabilidad de rechazar $H_0$ cuando $H_0$ es verdadera (error tipo I)\n",
    "\n",
    "## Tamaño de Muestra\n",
    "\n",
    "\n",
    "$$\n",
    "N = \\frac{(Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot 2p(1-p)}{\\Delta^2}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- **$Z_{\\alpha/2}$** es el umbral para rechazar $H_0$ (ej. 1.96 para $\\alpha = 0.05$.\n",
    "- **$Z_{\\beta}$** se elige según la potencia deseada (ej. 0.84 para 80% de potencia).\n",
    "- **$p(1-p)$** es la varianza de la tasa de conversión.\n",
    "- **$\\Delta$** es la diferencia esperada entre A y B ($p_B - p_A$).\n",
    "\n",
    "## Con StatsModels\n",
    "\n",
    "```python\n",
    "PA = 0.1\n",
    "PB = 0.12\n",
    "\n",
    "N = sms.proportion_effectsize(PA, PB)  # Efecto\n",
    "sample_size = sms.NormalIndPower().solve_power(effect_size=N, alpha=0.05, power=0.8, ratio=1)\n",
    "sample_size\n",
    "```\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9f03f",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Es un modelo en el que se van construyendo árboles de decisión de manera iterativa, donde el modelo aprende con los residuales del árbol anterior, y con ello, se actualiza haciendo un modelo para equivocarse menos. Al igual que en el descenso en gradiente, para actualizar el los árboles con cada iteración se utiliza el factor $\\alpha$ de learning rate y se repite hasta converger.\n",
    "\n",
    "La predicción del modelo sería de la siguiente manera:\n",
    "\n",
    "$$F_m(x) = F_{m - 1}(x) + \\nu (\\text{nuevo árbol})$$\n",
    "\n",
    "La predicción del árbol anterior será el promedio de los residuales de cada hoja.\n",
    "\n",
    "En clasificación, la predicción estaría en términos de log-odds.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fea2a",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Este modelo es la versión optimizada del Gradient Boosting. En este es un modelo aditivo de árboles, en otras palabras, el modelo final se construye sumando árboles, de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\hat{y}_i^{(t)}$ es la predicción del ejemplo $i$ en la iteración $t$\n",
    "- $f_t(x_i)$ es el nuevo árbol que se entrena en la iteración $t$\n",
    "\n",
    "Con cada iteración, se busca que el nuervo árbol $f_t$ minimice la contribución marginal a la función de pérdida total:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} =\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)}) + \\sum_{k=1}^{t-1} \\Omega(f_k)\n",
    "}_{\\text{Parte ya construida (constante en esta iteración)}} +\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) - l(y_i, \\hat{y}_i^{(t-1)}) \\right] + \\Omega(f_t)\n",
    "}_{\\text{Lo que añade el nuevo árbol $f_t$}}\n",
    "$$\n",
    "\n",
    "Donde $\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2$ es una penalización por la complejidad del árbol $f_k$\n",
    "\n",
    "Dado que esta función de pérdada es muy compleja, se utiliza la expansión de Taylor para aproximar esta función por medio de polinomios, y de esta manera hacerla más facil de optimizar:\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(a) + f'(a)(x - a) + \\frac{1}{2} f''(a)(x - a)^2\n",
    "$$\n",
    "\n",
    "Despues de áplicar la expansión de Taylor en XGBoost y utilizando el gradiente y hessiano, se llega, a que la función que se optimiza en cada iteración es:\n",
    "\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Cada nuevo árbol $f_t$ asigna un valor constante $w_j$, entonces, sustituyendo esto en la antigua fución objetivo y escribiendo $\\Omega(f)$ explícitamente:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "\\sum_{i \\in R_j} g_i w_j + \\frac{1}{2} \\sum_{i \\in R_j} h_i w_j^2\n",
    "\\right] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "Simplificando:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$G_j = \\sum_{i \\in R_j} g_i$\n",
    "\n",
    "$H_j = \\sum_{i \\in R_j} h_i$\n",
    "\n",
    "Luego de minimizar el término por hoja, derivando con respecto a $w_j$, llegamos al siguiente output value:\n",
    "\n",
    "$$\n",
    "\\textbf{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Luego se calcula el similarity score para evaluar cuanto mejora la pérdida por haber utilizado el óptimo:\n",
    "\n",
    "$$\n",
    "\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Gracias al similarity score ahora podemos calcular el Gain, y esta medida nos ayudará a ver si es conveniente realizar un split, es decir, aumentar el grado de complejidad de nuestro árbol:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\left( \\text{Similarity}_\\text{izq} + \\text{Similarity}_\\text{der} - \\text{Similarity}_\\text{padre} \\right) - \\gamma\n",
    "$$\n",
    "\n",
    "Luego de realizar todo lo anterior, podremos realizar la predicción final, que consiste en realizar la suma de todos los árboles si es regresión o aplicar la función sigmoide si es clasificación:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = F_0(x_i) + \\sum_{t=1}^{M} f_t(x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_i = \\frac{1}{1 + e^{-\\hat{y}_i}}\n",
    "$$\n",
    "\n",
    "## Resumen\n",
    "\n",
    "| Concepto            | Fórmula                                                       | Interpretación                                |\n",
    "|---------------------|----------------------------------------------------------------|------------------------------------------------|\n",
    "| Output value        | $w_j = -\\frac{G_j}{H_j + \\lambda}$                         | Valor óptimo que predice una hoja             |\n",
    "| Similarity score    | $\\frac{G_j^2}{H_j + \\lambda}$                              | Calidad de una hoja                           |\n",
    "| Gain (split)        | Ver fórmula anterior                                           | Reducción esperada en la pérdida si se divide |\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc680f78",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "Es un modelo, donde a diferencia de XGBoost, es más ligero computacionalmente y más rápido, debido a que suele funcionar con bins y así como tambien, generar árboles que no sean simétricos, con mayor o menor profundidad en ciertas ramas.\n",
    "\n",
    "Gracias a este crecimiento asimétrico, este modelo puede encontrar mejores cortes en los nodos, pero esto tambien puede llevar a que sea más propenso al overfitting.\n",
    "\n",
    "A diferencia de XGBoost, que construye árboles nivel por nivel (level-wise), LightGBM crece los árboles de forma hoja por hoja (leaf-wise)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abeb51",
   "metadata": {},
   "source": [
    "# CatBoost\n",
    "\n",
    "Tambien es un modelo de gradient boosting, sin embargo, este destaca por su grán capacidad de manejo de variables categóricas sin realizar encoding manual. Este modelo tambien genera árboles simétricos, lo que lo vuelve más robusto ante el overfitting.\n",
    "\n",
    "La forma en la que CatBoost maneja las variables categóricas funciona de la siguiente manera:\n",
    "\n",
    "1. Ordena las filas de forma aleatoria.\n",
    "2. Para cada fila se codifica la categoría con el promedio del target acumulato hasta ese momento con las observaciones anteriores.\n",
    "\n",
    "Dado que nunca se incluye el target de la fila actual, sino de la enteror, esto no introduce data leakege en el modelo.\n",
    "\n",
    "Esto es realizado con varias permutaciones del dataset, para al final promediar los resultados. Con esto se logra un encoding estable.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5713ca",
   "metadata": {},
   "source": [
    "# Comparativa de modelos de gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2e105",
   "metadata": {},
   "source": [
    "| Característica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | Rápido, pero más lento que LightGBM y CatBoost             | 🔥 Muy rápido gracias a histogramas y leaf-wise growth      | Rápido, aunque un poco más lento que LightGBM                  |\n",
    "| **Precisión**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categóricas                        |\n",
    "| **Variables categóricas**   | ❌ No las maneja (requiere encoding manual)                | ❌ No las maneja (requiere encoding manual)                 | ✅ Soporte nativo + regularización secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ✅ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ✅ Automático                                               | ✅ Automático                                                | ✅ Automático                                                   |\n",
    "| **Soporte GPU**             | ✅ Sí (bastante estable)                                   | ✅ Sí (muy rápido)                                           | ✅ Sí (algo más limitado)                                      |\n",
    "| **Instalación**             | Fácil (`pip install xgboost`)                             | Fácil (`pip install lightgbm`)                              | Un poco más pesada (`pip install catboost`)                   |\n",
    "| **Documentación**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacción con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ✅ Neutral                                                  | ✅ Neutral                                                   | ⚠️ Sensible (por codificación secuencial)                      |\n",
    "\n",
    "## Situaciones de cada modelo\n",
    "\n",
    "| Situación                                                  | Recomendación                                      |\n",
    "|------------------------------------------------------------|----------------------------------------------------|\n",
    "| Dataset tabular pequeño o mediano                          | ✅ XGBoost o CatBoost                               |\n",
    "| Dataset grande, muchas variables numéricas                 | ✅ LightGBM                                         |\n",
    "| Muchas variables categóricas sin preprocesamiento          | ✅ CatBoost (manejo nativo y robusto)              |\n",
    "| Quieres algo robusto y estable con buen soporte            | ✅ XGBoost (muy probado en producción y Kaggle)     |\n",
    "| Entrenamiento rápido con buen desempeño                    | ✅ LightGBM                                         |\n",
    "| Quieres interpretabilidad con SHAP                         | ✅ Cualquiera, pero CatBoost da mejores resultados con categóricas |\n",
    "| Necesitas buen rendimiento sin mucho tuning                | ✅ CatBoost (buenos defaults)                       |\n",
    "| Ya tienes pipeline con OneHot/Target Encoding              | ✅ XGBoost o LightGBM                               |\n",
    "| Tuning automático (Optuna, GridSearchCV, etc.)             | ✅ LightGBM (rápido y convergente)                  |\n",
    "| Producción en sistemas legacy o APIs bien documentadas     | ✅ XGBoost (mayor madurez, más integración)         |\n",
    "| Clasificación multi-label o problemas no estándar          | ✅ XGBoost (soporte más flexible)                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c81e9",
   "metadata": {},
   "source": [
    "# Partial dependece plots\n",
    "\n",
    "Dejas una variable fija y sacas el promedio de la predicción y sirve para ver como afecta esa variable a la predicción (relación de las variables)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8e5c3",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "## Weight\n",
    "\n",
    "Veces que se usa una variable en el modelo\n",
    "\n",
    "## Gain\n",
    "\n",
    "En promedio cuanto baja la función de pérdida una variable\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923399b",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
