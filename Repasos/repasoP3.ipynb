{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b0530c",
   "metadata": {},
   "source": [
    "# √Årboles de decisi√≥n\n",
    "\n",
    "Son modelos predictivos, que gracias a su estructura de niveles y nodos, se pueden separar los datos, de tal manera, que se compara si tu dato en cuesti√≥n sobrepasa el umbral establecido en una caracter√≠stica espec√≠fica. \n",
    "\n",
    "## Regresi√≥n\n",
    "\n",
    "Primero el algoritmo utiliza todas las caracter√≠sticas y umbrales de divisi√≥n posibles, y con ello, seleccionar los umbrales que logren la mayor reducci√≥n de varianza.\n",
    "\n",
    "$$\n",
    "\\text{Reducci√≥n de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Esta formula compara la varianza total de las ramas con la varianza despues de ser ponderada por la cantidad de datos en cada rama.\n",
    "\n",
    "## Clasificaci√≥n\n",
    "\n",
    "Igual que el de regresi√≥n, el algoritmo utiliza todas las caracter√≠sticas y umbrales de divisi√≥n posibles, y con ello, seleccionar que los umbrales que, ahora en este caso, logren la mayor reducci√≥n de impureza, calculada por medio de Gini o la Entrop√≠a.\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum p_i^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entrop√≠a = -\\sum p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Impureza} = \\text{Impureza Nodo Padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Esta formula compara la impureza del nodo padre con la impureza ponderada por la cantidad de datos en cada rama.\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "+ F√°cilmente interpretables gracias a las reglas y umbrales establecidos en el √°rbol de decisi√≥n. \n",
    "+ No es necesario escalar los datos\n",
    "\n",
    "## Desventajas\n",
    "\n",
    "+ Estos modelos pueden tender al overfitting debido a que se puede llegar a crear un √°rbol tan complejo, que crezca tanto que memorice los datos.\n",
    "+ No tan buen poder predictivo.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c4821",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Utilizando el m√©todo de bootstrap, se realiza un muestreo aleatorio para crear varios √°rboles diferentes. Estos √°rboles son entrenados y para realizar las predicciones cambiar√° dependiendo de si es regresi√≥n o clasificaci√≥n. \n",
    "\n",
    "+ Si es regresi√≥n se promediar√° la predicci√≥n de todos los √°rboles creados.\n",
    "+ Si es de clasificaci√≥n, se toma la categor√≠a m√°s repetida.\n",
    "\n",
    "## Ventajas\n",
    "\n",
    "+ Mayor poder predictivo de comparado a los arboles de decisi√≥n normales.\n",
    "+ Gracias al muestreo aleatorio se reduce en gran medida el overfitting.\n",
    "+ No es necesario escalamiento de datos.\n",
    "\n",
    "## Desventajas\n",
    "\n",
    "+ Pierde interpretabilidad.\n",
    "+ Es mas costoso computacionalmente.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0e7e76",
   "metadata": {},
   "source": [
    "# Prueba de poder\n",
    "\n",
    "Es una t√©cnica utilizada para determinar el tama√±o de muestra necesario para lograr observar un efecto con una probabilidad determinada, as√≠ como tambie, para ver que tan probable es que mi estudio detecte un efecto real (si es que existe).\n",
    "\n",
    "## Error tipo I $\\alpha$\n",
    "\n",
    "Falso positivo: rechazamos $H_0$ cuando en realidad es verdadera\n",
    "\n",
    "## Error tipo II ($\\beta$)\n",
    "\n",
    "Falso negativo: No rechazamos $H_0$ cuando en realidad $H_A$ es la verdadera\n",
    "\n",
    "| Tipo de Error | Decisi√≥n Tomada  | Realidad         | Consecuencia |\n",
    "|--------------|-----------------|-----------------|--------------|\n",
    "| **Tipo I $\\alpha$** | Cambiamos a rojo | No hab√≠a diferencia | Cambio innecesario, p√©rdida de recursos |\n",
    "| **Tipo II $\\beta$** | No cambiamos a rojo | Rojo era mejor | Perdemos una oportunidad de mejora |\n",
    "\n",
    "## Elementos clave\n",
    "- Potencia : (1 - $\\beta$) Es la probabilidad de rechazar $H_0$ cuando $H_A$ es negativa (Error tipo II). Usualmente queremos tener $\\beta$ en 80%\n",
    "- Nivel de significancia: $\\alpha$ Es la probabilidad de rechazar $H_0$ cuando $H_0$ es verdadera (error tipo I)\n",
    "\n",
    "## Tama√±o de Muestra\n",
    "\n",
    "\n",
    "$$\n",
    "N = \\frac{(Z_{\\alpha/2} + Z_{\\beta})^2 \\cdot 2p(1-p)}{\\Delta^2}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- **$Z_{\\alpha/2}$** es el umbral para rechazar $H_0$ (ej. 1.96 para $\\alpha = 0.05$.\n",
    "- **$Z_{\\beta}$** se elige seg√∫n la potencia deseada (ej. 0.84 para 80% de potencia).\n",
    "- **$p(1-p)$** es la varianza de la tasa de conversi√≥n.\n",
    "- **$\\Delta$** es la diferencia esperada entre A y B ($p_B - p_A$).\n",
    "\n",
    "## Con StatsModels\n",
    "\n",
    "```python\n",
    "PA = 0.1\n",
    "PB = 0.12\n",
    "\n",
    "N = sms.proportion_effectsize(PA, PB)  # Efecto\n",
    "sample_size = sms.NormalIndPower().solve_power(effect_size=N, alpha=0.05, power=0.8, ratio=1)\n",
    "sample_size\n",
    "```\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9f03f",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "Es un modelo en el que se van construyendo √°rboles de decisi√≥n de manera iterativa, donde el modelo aprende con los residuales del √°rbol anterior, y con ello, se actualiza haciendo un modelo para equivocarse menos. Al igual que en el descenso en gradiente, para actualizar el los √°rboles con cada iteraci√≥n se utiliza el factor $\\alpha$ de learning rate y se repite hasta converger.\n",
    "\n",
    "La predicci√≥n del modelo ser√≠a de la siguiente manera:\n",
    "\n",
    "$$F_m(x) = F_{m - 1}(x) + \\nu (\\text{nuevo √°rbol})$$\n",
    "\n",
    "La predicci√≥n del √°rbol anterior ser√° el promedio de los residuales de cada hoja.\n",
    "\n",
    "En clasificaci√≥n, la predicci√≥n estar√≠a en t√©rminos de log-odds.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0fea2a",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Este modelo es la versi√≥n optimizada del Gradient Boosting. En este es un modelo aditivo de √°rboles, en otras palabras, el modelo final se construye sumando √°rboles, de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\hat{y}_i^{(t)}$ es la predicci√≥n del ejemplo $i$ en la iteraci√≥n $t$\n",
    "- $f_t(x_i)$ es el nuevo √°rbol que se entrena en la iteraci√≥n $t$\n",
    "\n",
    "Con cada iteraci√≥n, se busca que el nuervo √°rbol $f_t$ minimice la contribuci√≥n marginal a la funci√≥n de p√©rdida total:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} =\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n l(y_i, \\hat{y}_i^{(t-1)}) + \\sum_{k=1}^{t-1} \\Omega(f_k)\n",
    "}_{\\text{Parte ya construida (constante en esta iteraci√≥n)}} +\n",
    "\\underbrace{\n",
    "\\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)) - l(y_i, \\hat{y}_i^{(t-1)}) \\right] + \\Omega(f_t)\n",
    "}_{\\text{Lo que a√±ade el nuevo √°rbol $f_t$}}\n",
    "$$\n",
    "\n",
    "Donde $\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2$ es una penalizaci√≥n por la complejidad del √°rbol $f_k$\n",
    "\n",
    "Dado que esta funci√≥n de p√©rdada es muy compleja, se utiliza la expansi√≥n de Taylor para aproximar esta funci√≥n por medio de polinomios, y de esta manera hacerla m√°s facil de optimizar:\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(a) + f'(a)(x - a) + \\frac{1}{2} f''(a)(x - a)^2\n",
    "$$\n",
    "\n",
    "Despues de √°plicar la expansi√≥n de Taylor en XGBoost y utilizando el gradiente y hessiano, se llega, a que la funci√≥n que se optimiza en cada iteraci√≥n es:\n",
    "\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "Cada nuevo √°rbol $f_t$ asigna un valor constante $w_j$, entonces, sustituyendo esto en la antigua fuci√≥n objetivo y escribiendo $\\Omega(f)$ expl√≠citamente:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "\\sum_{i \\in R_j} g_i w_j + \\frac{1}{2} \\sum_{i \\in R_j} h_i w_j^2\n",
    "\\right] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "Simplificando:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$G_j = \\sum_{i \\in R_j} g_i$\n",
    "\n",
    "$H_j = \\sum_{i \\in R_j} h_i$\n",
    "\n",
    "Luego de minimizar el t√©rmino por hoja, derivando con respecto a $w_j$, llegamos al siguiente output value:\n",
    "\n",
    "$$\n",
    "\\textbf{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Luego se calcula el similarity score para evaluar cuanto mejora la p√©rdida por haber utilizado el √≥ptimo:\n",
    "\n",
    "$$\n",
    "\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Gracias al similarity score ahora podemos calcular el Gain, y esta medida nos ayudar√° a ver si es conveniente realizar un split, es decir, aumentar el grado de complejidad de nuestro √°rbol:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\frac{1}{2} \\left( \\text{Similarity}_\\text{izq} + \\text{Similarity}_\\text{der} - \\text{Similarity}_\\text{padre} \\right) - \\gamma\n",
    "$$\n",
    "\n",
    "Luego de realizar todo lo anterior, podremos realizar la predicci√≥n final, que consiste en realizar la suma de todos los √°rboles si es regresi√≥n o aplicar la funci√≥n sigmoide si es clasificaci√≥n:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = F_0(x_i) + \\sum_{t=1}^{M} f_t(x_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p_i = \\frac{1}{1 + e^{-\\hat{y}_i}}\n",
    "$$\n",
    "\n",
    "## Resumen\n",
    "\n",
    "| Concepto            | F√≥rmula                                                       | Interpretaci√≥n                                |\n",
    "|---------------------|----------------------------------------------------------------|------------------------------------------------|\n",
    "| Output value        | $w_j = -\\frac{G_j}{H_j + \\lambda}$                         | Valor √≥ptimo que predice una hoja             |\n",
    "| Similarity score    | $\\frac{G_j^2}{H_j + \\lambda}$                              | Calidad de una hoja                           |\n",
    "| Gain (split)        | Ver f√≥rmula anterior                                           | Reducci√≥n esperada en la p√©rdida si se divide |\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc680f78",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "\n",
    "Es un modelo, donde a diferencia de XGBoost, es m√°s ligero computacionalmente y m√°s r√°pido, debido a que suele funcionar con bins y as√≠ como tambien, generar √°rboles que no sean sim√©tricos, con mayor o menor profundidad en ciertas ramas.\n",
    "\n",
    "Gracias a este crecimiento asim√©trico, este modelo puede encontrar mejores cortes en los nodos, pero esto tambien puede llevar a que sea m√°s propenso al overfitting.\n",
    "\n",
    "A diferencia de XGBoost, que construye √°rboles nivel por nivel (level-wise), LightGBM crece los √°rboles de forma hoja por hoja (leaf-wise)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abeb51",
   "metadata": {},
   "source": [
    "# CatBoost\n",
    "\n",
    "Tambien es un modelo de gradient boosting, sin embargo, este destaca por su gr√°n capacidad de manejo de variables categ√≥ricas sin realizar encoding manual. Este modelo tambien genera √°rboles sim√©tricos, lo que lo vuelve m√°s robusto ante el overfitting.\n",
    "\n",
    "La forma en la que CatBoost maneja las variables categ√≥ricas funciona de la siguiente manera:\n",
    "\n",
    "1. Ordena las filas de forma aleatoria.\n",
    "2. Para cada fila se codifica la categor√≠a con el promedio del target acumulato hasta ese momento con las observaciones anteriores.\n",
    "\n",
    "Dado que nunca se incluye el target de la fila actual, sino de la enteror, esto no introduce data leakege en el modelo.\n",
    "\n",
    "Esto es realizado con varias permutaciones del dataset, para al final promediar los resultados. Con esto se logra un encoding estable.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5713ca",
   "metadata": {},
   "source": [
    "# Comparativa de modelos de gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc2e105",
   "metadata": {},
   "source": [
    "| Caracter√≠stica              | **XGBoost**                                                | **LightGBM**                                               | **CatBoost**                                                  |\n",
    "|-----------------------------|------------------------------------------------------------|-------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **Velocidad**               | R√°pido, pero m√°s lento que LightGBM y CatBoost             | üî• Muy r√°pido gracias a histogramas y leaf-wise growth      | R√°pido, aunque un poco m√°s lento que LightGBM                  |\n",
    "| **Precisi√≥n**               | Alta                                                       | Alta, a veces mejor con buen tuning                         | Muy alta, especialmente con categ√≥ricas                        |\n",
    "| **Variables categ√≥ricas**   | ‚ùå No las maneja (requiere encoding manual)                | ‚ùå No las maneja (requiere encoding manual)                 | ‚úÖ Soporte nativo + regularizaci√≥n secuencial                  |\n",
    "| **Uso de memoria**          | Moderado                                                   | ‚úÖ Muy eficiente (binning)                                   | Similar a XGBoost                                              |\n",
    "| **Manejo de missing values**| ‚úÖ Autom√°tico                                               | ‚úÖ Autom√°tico                                                | ‚úÖ Autom√°tico                                                   |\n",
    "| **Soporte GPU**             | ‚úÖ S√≠ (bastante estable)                                   | ‚úÖ S√≠ (muy r√°pido)                                           | ‚úÖ S√≠ (algo m√°s limitado)                                      |\n",
    "| **Instalaci√≥n**             | F√°cil (`pip install xgboost`)                             | F√°cil (`pip install lightgbm`)                              | Un poco m√°s pesada (`pip install catboost`)                   |\n",
    "| **Documentaci√≥n**           | Excelente                                                  | Buena                                                       | Muy buena                                                     |\n",
    "| **Interacci√≥n con sklearn** | Muy buena                                                  | Muy buena                                                   | Muy buena                                                     |\n",
    "| **Tolerancia al orden**     | ‚úÖ Neutral                                                  | ‚úÖ Neutral                                                   | ‚ö†Ô∏è Sensible (por codificaci√≥n secuencial)                      |\n",
    "\n",
    "## Situaciones de cada modelo\n",
    "\n",
    "| Situaci√≥n                                                  | Recomendaci√≥n                                      |\n",
    "|------------------------------------------------------------|----------------------------------------------------|\n",
    "| Dataset tabular peque√±o o mediano                          | ‚úÖ XGBoost o CatBoost                               |\n",
    "| Dataset grande, muchas variables num√©ricas                 | ‚úÖ LightGBM                                         |\n",
    "| Muchas variables categ√≥ricas sin preprocesamiento          | ‚úÖ CatBoost (manejo nativo y robusto)              |\n",
    "| Quieres algo robusto y estable con buen soporte            | ‚úÖ XGBoost (muy probado en producci√≥n y Kaggle)     |\n",
    "| Entrenamiento r√°pido con buen desempe√±o                    | ‚úÖ LightGBM                                         |\n",
    "| Quieres interpretabilidad con SHAP                         | ‚úÖ Cualquiera, pero CatBoost da mejores resultados con categ√≥ricas |\n",
    "| Necesitas buen rendimiento sin mucho tuning                | ‚úÖ CatBoost (buenos defaults)                       |\n",
    "| Ya tienes pipeline con OneHot/Target Encoding              | ‚úÖ XGBoost o LightGBM                               |\n",
    "| Tuning autom√°tico (Optuna, GridSearchCV, etc.)             | ‚úÖ LightGBM (r√°pido y convergente)                  |\n",
    "| Producci√≥n en sistemas legacy o APIs bien documentadas     | ‚úÖ XGBoost (mayor madurez, m√°s integraci√≥n)         |\n",
    "| Clasificaci√≥n multi-label o problemas no est√°ndar          | ‚úÖ XGBoost (soporte m√°s flexible)                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c81e9",
   "metadata": {},
   "source": [
    "# Partial dependece plots\n",
    "\n",
    "Dejas una variable fija y sacas el promedio de la predicci√≥n y sirve para ver como afecta esa variable a la predicci√≥n (relaci√≥n de las variables)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c8e5c3",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "## Weight\n",
    "\n",
    "Veces que se usa una variable en el modelo\n",
    "\n",
    "## Gain\n",
    "\n",
    "En promedio cuanto baja la funci√≥n de p√©rdida una variable\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923399b",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
