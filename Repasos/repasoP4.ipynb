{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1df2df",
   "metadata": {},
   "source": [
    "# Shap Values\n",
    "\n",
    "Los Shap Values son valores calculados para cada variable del modelo que miden la contribución marginal que tienen a la predicción de un modelo. Consiste en tomar todas las permutaciones posibles de una observación que se quiere predecir con el modelo, y se reemplazaría de uno por uno ese valor en los datos originales, de tal manera, que dejarías uno fijo para ver como afecta eso a la predicción del modelo. Y así sucesivamente con todas las variables hasta encontrar la contribución de cada variable del modelo.\n",
    "\n",
    "Primero se inicia de un modelo base el cual llamado \"Baseline\" el cual es la predicción promedio del dataset. Imaginando que de la observación $[x_1, x_2, x_3]$ se escoge la permutación $[x_2, x_1, x_3]$, entonces la forma de llegar a las contribuciones de de esta permutación sería:\n",
    "\n",
    "1. Se comienza con el baseline $f(0)$\n",
    "2. Se agrega $x_2$ y se obtiene su contribución: $f({x_2}) - f(0)$\n",
    "3. Se agrega $x_1$ y se obtiene su contribución: $f({x_2, x_1}) - f({x_2})$\n",
    "4. Se agrega $x_3$ y se obtiene su contribución: $f({x_2, x_1, x_3}) - f({x_2, x_1})$\n",
    "\n",
    "Este proceso se repetiría para todas las distintas permutaciones de la observación y al final el Shap value de cada una sería un promedio ponderado de sus contribuciones en todas las permutaciones, lo que se define como $\\text{SHAP} x_i $ . Al final se llegaría a que la predicción del modelo para una observación es:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2, x_3) = f(0) + \\text{SHAP} x_1 + \\text{SHAP} x_2 + \\text{SHAP} x_3\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764c406",
   "metadata": {},
   "source": [
    "# Kmeans\n",
    "\n",
    "Es un algoritmo utilizado para agrupar tus datos (o hacer clústeres) de tal manera que los datos dentro de cada clúster sean lo más parecidos entre sí, y tambien, diferentes a los de los otros clusters. El funcionamiento de este algoritmo se basa en generar aleatoriamente $n$ cantidad de centroides, posteriormente cada dato se asignará al centroide más cercano (medido con la distacia euclidiana) y luego se repetiría tantas veces como sea necesario recalculando los centroides como el promedio de los puntos asignados a cada grupo hasta que los centroides dejen de cambiar y entonces haya convergido el algoritmo.\n",
    "\n",
    "La fórmula con la que se calcula la distancia euclidiana, la cual se necesita para recalcular los centroides es la siguiente:\n",
    "\n",
    "$$\n",
    "\\|x_i - \\mu_k\\|^2 = \\sum_{j=1}^n (x_{ij} - \\mu_{kj})^2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "+ $x_i$: Es el dato en cuestión.\n",
    "+ $\\mu_k$: El centroide.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddfe17d",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "El Principal Component Analysis es una técnica utilizada para reducir la dimensionalidad de un conjunto de datos, mientras se trata de mantener la mayor cantidad de varianza original. Para realizarlo, primero se comienza con la proyección de los datos sobre un componente:\n",
    "\n",
    "$$\n",
    "\\text{Proy}_{u_1}(x_i) = u_1^T x_i\n",
    "$$\n",
    "\n",
    "De estas proyecciones se calcula la media como:\n",
    "\n",
    "$$\n",
    "u_1^T \\bar{x}\n",
    "$$\n",
    "\n",
    "Entonces, la varianza de los datos proyectados sería:\n",
    "\n",
    "\n",
    "La varianza de los datos proyectados sobre $u_1$ es:\n",
    "\n",
    "$$\n",
    "\\text{Varianza} = \\frac{1}{N} \\sum_{n=1}^N \\left( u_1^T x_n - u_1^T \\bar{x} \\right)^2\n",
    "$$\n",
    "\n",
    "Realizando las operaciones correspondientes, podemos simplificar a \n",
    "\n",
    "$$\n",
    "\\text{Varianza} = u_1^T S u_1\n",
    "$$\n",
    "\n",
    "* Donde $S$ es la matriz de covarianzas de los datos.\n",
    "\n",
    "A lo que se quiere llegar con esto es proyectar los datos sobre un $u_1$, el cual maximice la varianza capturada, lo que ayudaría a poder explicar de la mejor manera los datos originales en una dimensión menor. Por lo tanto, habría que realizar un proceso de optimización, tal que:\n",
    "\n",
    "$$\n",
    "\\max_{u_1} u_1^T S u_1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{sujeto a:} \\|u_1\\| = 1\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $S$ es la matriz de covarianza de los datos.\n",
    "- $\\|u_1\\| = 1$ asegura que $u_1$ sea un vector de norma 1.\n",
    "\n",
    "Para resolverlo podemos plantear si siguiente lagrangiano:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(u_1, \\lambda) = u_1^T S u_1 - \\lambda (u_1^T u_1 - 1)\n",
    "$$\n",
    "\n",
    "Luego de derivar e igualar a cero, se llega a que:\n",
    "\n",
    "$$\n",
    "S u_1 = \\lambda u_1\n",
    "$$\n",
    "\n",
    "Con este resultado nos podemos dar cuenta de que, lo que estamos resolviendo es un problema de eigenvalores y eigenvectores, pues $u_1$ sería un vector propio de la matriz $S$ de covarianzas y $\\lambda$ un valor propio. \n",
    "\n",
    "Con esto, $u_1$ entonces sería el primer componente principal del PCA y correspondería al vector propio que corresponde al eigenvalor más grande de $S$. La segunda componente principal sería entonces un vector propio que resuelva para el siguiente valor propio más grande y así continuaría el análsis para la cantidad de componentes deseados.\n",
    "\n",
    "En resumen, el PCA consiste en encontrar un vector propio (una dirección) en donde al proyectar tus datos, la varianza sea máxima. Lo que ayuda a poder mantener la máxima cantidad de varianza en tus datos al momento de reducir de dimensiones.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfd9e9",
   "metadata": {},
   "source": [
    "# Conditional average treatment effect (CATE)\n",
    "\n",
    "Para una covariable \\( X = x \\), el CATE se define como:\n",
    "\n",
    "$$\n",
    "\\text{CATE}(x) = \\mathbb{E}[Y(1) - Y(0) \\mid X = x]\n",
    "$$\n",
    "\n",
    "Es decir, el efecto esperado del tratamiento para individuos con características \\( X = x \\).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2ed3a0",
   "metadata": {},
   "source": [
    "# S-Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92770af6",
   "metadata": {},
   "source": [
    "Un S-Learner consiste en entrenar un modelo con los datos originales de entrenamiento. Posteriormente hay que elegir la variable que se quisiera analizar, pues a esta variable se la asignarían multiples valores y el modelo tendría que realizar predicciones sobre estos nuevos valores. Luego de realizar las predicciones se podrá ver como el cambiar una variable puede afectar a la predicción del modelo. \n",
    "\n",
    "Dependiendo de si los datos son discretos o continuos se aplicaría de maneras diferentes:\n",
    "\n",
    "+ Si los datos son continuos se haría lo que se mencionó anteriormente, generar una lista de valores sobre los que se quisiera probar el impacto en la variable a predecir. Posteriormente predecirlo con el modelo, y con ello ver el efecto causal que tiene esa variable midiendo el CATE para cada predicción.\n",
    "\n",
    "+ Si los datos son discretos, entonces, la variable sobre la que se quiere medir el efecto habría que modificarla de tal manera que se puedan tener los datos con un tratamiento o no, es decir, en donde todos sean 1 y todos sean 0. Con esto, nuevamente se realizarían predicciones con ambos grupos de datos y se obtendría el CATE para medir el efecto de aplicar este tratamiento en los datos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253d12ee",
   "metadata": {},
   "source": [
    "# T-Learner\n",
    "\n",
    "En lugar tener un solo modelo para predecir con los datos modificados, ahora se crearían 2 diferentes datasets, uno en donde se tiene el tratamiento y otro donde no se tiene. De igual manera se crearían 2 diferentes modelos, uno entrenado con los datos con tratamiento y otro entrenado con los datos sin tratamiento. Para finalizar se realizan las predicciones a los datos originales del conjunto de prueba con ambos modelos. El objetivo de esto es ver como cambia predecir los mismos datos con 2 modelos diferentes, en los que uno de ellos fue entrenado con tratamiento y el otro no.\n",
    "\n",
    "Nuevamente se calcularían las diferencias entre las predicciones del modelo con tratamiento y las del modelo sin tratamiento. Esta resta sería el CATE del modelo, y nuevamente solo quedaría comprobar si hubo efecto o no. Si las predicciones son similares, entonces significaría que esa variable no tiene efecto al momento de predecir, pues sería casi lo mismo si fuera 1 o 0.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e837ac",
   "metadata": {},
   "source": [
    "# X-Learner\n",
    "\n",
    "Al igual que los anteriores, este tambien es un modelo para medir el efecto causal. Este modelo solo serviría cuando el tratamiento es para una variable binaria. Para desarrollarlo primero habría que crear 2 modelos entrenados con y sin tratamiento al igual que en el T-Learner. Con estos modelos ahora hay que calcular los efectos individuales contrafactuales $D_0$ y $D_1$, los cuales se pueden definir de la siguiente manera:\n",
    "\n",
    "+ $D_0$ sería lo que pasó menos lo que pasaría menos lo que hubiera pasado sin tratamiento, entonces:\n",
    "\n",
    "$$\n",
    "D^0_i = Y_i - M_0(X_i)\n",
    "$$\n",
    "\n",
    "+ $D_1$ sería lo que hubiera pasado con tratamiento menos lo que pasó, entonces:\n",
    "\n",
    "$$\n",
    "D^1_i = M_1(X_i) - Y_i \n",
    "$$\n",
    "\n",
    "Con los efectos $D_0$ y $D_1$ definidos, ahora habría que entrenar 2 modelos $\\tau$ nuevos, con el objetivo de que cada uno pueda predecir el CATE o efecto causal condicional dentro de cada grupo:\n",
    "\n",
    "+ $\\tau_{0} (x) $ sería un modelo entrado con los datos con tratamiento aplicado y trataría de predecir los efectos $D0$.\n",
    "\n",
    "+ $\\tau_{1} (x) $ sería un modelo entrado con los datos sin tratamiento y se trataría de predecir los efectos $D1$.\n",
    "\n",
    "Con todo lo anterior se calcularía el propensity score, el cual es la probabilidad que tendría una observación de recibir un tratamiento condicional. Para calcularlo, nuevamente se puede crear un nuevo modelo, el cual se entrenaría utilizando los datos de prueba sin la variable que se está analizando, y con ello tratar de predecir la probabilidad de que se reciba tratamiento.\n",
    "\n",
    "Para finalizar, habría que calcular ahora el CATE. Como se mencionó anteriormente, se utilizaría  $\\tau_{0} (x) $ y $\\tau_{1} (x) $  para encontrar los CATEs, por lo que se tendrían 2 cates para una observación, entonces con ello se calcularía el CATE con un promedio ponderado por el propensity score, el cual le daría más peso a al modelo opuesto al cual pertenecería. La forma de calcularlo es:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}(x) = (1 - \\hat{e}(x)) \\cdot \\hat{\\tau}_0(x) + \\hat{e}(x) \\cdot \\hat{\\tau}_1(x)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\hat{e}(x)$ sería el propensity score para cada observación, es decir, la probabilidad de recibir tratamiento."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
