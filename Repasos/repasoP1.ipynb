{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validación\n",
    "\n",
    "Consiste en dividir nuestros datos en un conjunto de \"train\" y \"test\" para entrenar un modelo con el conjunto de \"train\" y probar el desempeño de este modelo en datos nuevos, que en este caso son el conjunto de \"test\". Además, hacer esta división nos sirve detectar el overfitting o memorización del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "# Decenso en gradiente\n",
    "\n",
    "Es un algoritmo que a base de iterar encontrará las $\\beta$ que minimicen la función de pérdida del modelo. \n",
    "\n",
    "Definamos la regresión lineal como: \n",
    "\n",
    "$$\\hat{y}=\\beta_0 + \\beta_1x_1$$\n",
    "\n",
    "Entonces la función de pérdida el error cuadrático es:\n",
    "\n",
    "$$L = \\frac{1}{2} \\Sigma (\\hat{y}-y)^2$$\n",
    "\n",
    "Dado que está elevada al cuadrado, sabemos que es una parábola y tiene un mínimo. Entonces podemos evaluar de manera recursiva y utilizar el descenso en gradiente. Para ello, hay que repetir hasta converger lo siguiente:\n",
    "\n",
    "$$\\beta_i = \\beta_i - \\alpha \\frac{\\partial L}{\\partial \\beta_i} $$\n",
    "\n",
    "Donde $\\alpha$ es la tasa de aprendizaje y nos ayuda a determinar que tanto va a avanzar con cada actualización. Además restar el $\\alpha$ nos sirve para que también evalue el otro lado de la parábola dado que cada iteración cambiaría de signo la pendiente y así llegar al mínimo.  \n",
    "\n",
    "Tambien existen función de pérdida con regularizaciones\n",
    "\n",
    "+ Ridge: $L = \\frac{1}{2} \\Sigma (\\hat{y}-y)^2 + \\Sigma \\beta^2$. Al elevarse las $\\beta$ al cuadrado hay penalizaciones para las mayores y así controla todos los coeficientes $\\beta$ hacíendolos mas pequeños.\n",
    "\n",
    "+ Lasso: $L = \\frac{1}{2} \\Sigma (\\hat{y}-y)^2 + \\Sigma |\\beta|$. Tambien penaliza los valores grandes de $\\beta$, pero al ser la suma de los valores absolutos esto puede llevar a que algunas $\\beta$ sean cero, y con ello, eliminarlas del modelo.\n",
    "\n",
    "---\n",
    "\n",
    "# Regresiones lineales simples\n",
    "\n",
    "Consiste en ajustar una recta en la forma: \n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "donde $\\beta_0$ es la el corte en el eje $y$ y $\\beta_1$ es la pendiente de la recta. Además, los coeficientes $\\beta$ tienen que ser aquellos que minimizen la función de pérdida que se define como $L = \\frac{1}{2} \\Sigma (\\hat{y}-y)^2$ donde $\\hat{y}$ son las predicciones y $y$ son los datos reales.\n",
    "\n",
    "---\n",
    "\n",
    "# Regresiones polinomiales\n",
    "\n",
    "La regresión polinomial, ahora no solo trata de ajustar rectas, sino polinomios tambien. La ecuación viene dada como: \n",
    "$$\\hat{y}= \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_n x^n$$\n",
    "\n",
    "donde $\\beta_0$ es el corte en el eje de las $y$, y las $\\beta$ son los coeficientes que corresponden a cada grado del polinomio. De igual forma, estos coeficientes $\\beta$ son tales que minimizan la función de pérdida, que se definen como $L = \\frac{1}{2} \\Sigma (\\hat{y}-y)^2$ donde $\\hat{y}$ son las predicciones y $y$ son los datos reales.\n",
    "\n",
    "---\n",
    "\n",
    "# Categorizar variables\n",
    "\n",
    "Es una herramienta que normalmente se utiliza para convertir variables no numéricas en numéricas. Por ejemplo, las variables dummy, que a partir de variables categóricas, creamos nuevas variables numéricas booleanas.\n",
    "\n",
    "---\n",
    "\n",
    "# Prueba de hipótesis\n",
    "\n",
    "Consiste en probar que el haber realizado un cambio haya sido significativo en algo. Por ejemplo, un grupo de personas se partirá, en donde a uno se les mostrará todo igual ('control') y a otro se les mostrará el cambio ('test'). El grupo de 'control' debe ser el que mas personas contenga.\n",
    "\n",
    "Una vez separados los grupos y hecho el cambio en el 'test' se recolectarán datos y se hará una prueba de hipótesis. La hipótesis nula será que la diferencia no exista, y la alterna es que si existe una diferencia. Habiendo calculado el P-Value, podemos concluir si el cambio realizado en el 'test' tuvo un impacto, y concluir si vale la pena hacer ese cambio o no.\n",
    "\n",
    "En las regresiones es útil porque nos ayuda a ver si las variables que estamos utilizando son significativas para el modelo y así poder decidir se es bueno utilizarlas o no.\n",
    "\n",
    "---\n",
    "\n",
    "# Análisis vibariados\n",
    "\n",
    "Se utilizan para observar las relaciones que tienen 2 variables entre sí, por ejemplo, ver si se relacionan de manera lineal, o mas bien en forma cuadrática, cúbica, etc. Gracias a estos análisis podemos decidir realizar modificaciones en nuestras variables para que tengan un mejor ajuste en el modelo.\n",
    "\n",
    "---\n",
    "\n",
    "# Teorema de Frisch-Waugh-Lovell\n",
    "\n",
    "Dada una regresión $\\hat{y}=\\beta_0+\\beta_1 x_1+\\beta_2 x_2+\\beta_3 x_3$ podemos crear una regresión nueva en donde no utilicemos a $x_1$ como predictora en el modelo:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_2 x_2 + \\theta_3 x_3$$\n",
    "\n",
    "Con esta regresión podemos obtener:\n",
    "\n",
    "$$\\text{resdiduales } y = y-\\hat{y}$$\n",
    "\n",
    "Estos residuales son lo que $x_2$ y $x_3$ no lograron explicar de $y$. \n",
    "\n",
    "Si posteriormente realizamos una regresión para tratar de predecir $x_1$ a partir de $x_2$ y $x_3$:\n",
    "\n",
    "$$\\hat{x_1} = \\gamma_0 + \\gamma_2 x_2 + \\gamma_3 x_3$$\n",
    "\n",
    "Con esta regresión podemos obtener:\n",
    "\n",
    "$$\\text{resdiduales } x = x-\\hat{x}$$\n",
    "\n",
    "Estos residuales son los que $x_2$ y $x_3$ no lograron explicar de $x_1$.\n",
    "\n",
    "Por último podemos realizar una regresión de los residuales de la forma:\n",
    "\n",
    "$$ y-\\hat{y} = \\beta_1 (x-\\hat{x}) $$\n",
    "\n",
    "Dado que $x_2$ y $x_3$ ya no están en esta regresión, podemos obtener el efecto causal que tiene $x_1$ sobre $y$ sin la necesidad de utilizar otras variables, sino que basta con los residuales de otras regresiones. Además el $\\beta_1$ obtenido en la regresión de los residuales será el mismo que en la regresión original.\n",
    "\n",
    "---\n",
    "\n",
    "# R2 score\n",
    "\n",
    "Es el porcentaje que nuestro modelo explica de la variabilidad de los datos. Por lo que un R2 cercano a 1 significa que gran parte de la variabilidad de los datos fue explicada por nuestro modelo.\n",
    "\n",
    "---\n",
    "\n",
    "# KNN\n",
    "\n",
    "Es un modelo que trata de predecir a partir de sus datos vecinos. El modelo funciona de la siguiente manera:\n",
    "1. Se define el número $k$ de vecinos.\n",
    "2. Se obtienen los $k$ vecinos mas cercanos (medidos con la distancia euclidiana) a nuestra nueva observación.\n",
    "3. La predicción para la nueva observación será el promedio los valores de estos vecinos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
