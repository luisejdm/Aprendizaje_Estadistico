{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d961bc0b",
   "metadata": {},
   "source": [
    "# Examen 4\n",
    "\n",
    "## Sección 1: Interpretabilidad con SHAP (25 puntos)\n",
    "\n",
    "**1.1** (Teoría, 10 pts)  \n",
    "**¿Qué representa un valor SHAP en el contexto de un modelo de machine learning? ¿Cuál es su fundamento teórico?**\n",
    "\n",
    "\n",
    "Los Shap Values son valores calculados para cada variable del modelo que miden la contribución marginal que tienen a la predicción de un modelo. Consiste en tomar todas las permutaciones posibles de una observación que se quiere predecir con el modelo, y se reemplazaría de uno por uno ese valor en los datos originales, de tal manera, que dejarías uno fijo para ver como afecta eso a la predicción del modelo. Y así sucesivamente con todas las variables hasta encontrar la contribución de cada variable del modelo.\n",
    "\n",
    "Primero se inicia de un modelo base el cual llamado \"Baseline\" el cual es la predicción promedio del dataset. Imaginando que de la observación $[x_1, x_2, x_3]$ se escoge la permutación $[x_2, x_1, x_3]$, entonces la forma de llegar a las contribuciones de de esta permutación sería:\n",
    "\n",
    "1. Se comienza con el baseline $f(0)$\n",
    "2. Se agrega $x_2$ y se obtiene su contribución: $f({x_2}) - f(0)$\n",
    "3. Se agrega $x_1$ y se obtiene su contribución: $f({x_2, x_1}) - f({x_2})$\n",
    "4. Se agrega $x_3$ y se obtiene su contribución: $f({x_2, x_1, x_3}) - f({x_2, x_1})$\n",
    "\n",
    "Este proceso se repetiría para todas las distintas permutaciones de la observación y al final el Shap value de cada una sería un promedio ponderado de sus contribuciones en todas las permutaciones, lo que se define como $\\text{SHAP} x_i $ . Al final se llegaría a que la predicción del modelo para una observación es:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2, x_3) = f(0) + \\text{SHAP} x_1 + \\text{SHAP} x_2 + \\text{SHAP} x_3\n",
    "$$\n",
    "\n",
    "Esta ecuación implica que la suma del promedio del modelo con los SHAP values debería de ser igual a la predicción que realiza el modelo.\n",
    "\n",
    "\n",
    "**1.2** (Cálculo, 10 pts)  \n",
    "**Un modelo predice que un cliente tendrá una probabilidad de impago del 0.78. El valor esperado del modelo es 0.5. Los SHAP values para tres variables son:**\n",
    "- Edad: +0.10  \n",
    "- Ingreso mensual: -0.05  \n",
    "- Historial crediticio: +0.23  \n",
    "\n",
    "**¿La suma es consistente con la predicción? Explica.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9720f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799999999999999"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5+0.1+0.23-0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5000542",
   "metadata": {},
   "source": [
    "Al sumar el valor esperado del modelo y los SHAP values se obtiene 0.77999999 = 0.78, por lo tanto, la suma si es consistente con la predicción, pues la suma de los shaps balancea los resultados para que que la suma del promedio con los shap sea igual a la predicción. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727a0f9",
   "metadata": {},
   "source": [
    "## Sección 2: K-Means Clustering (20 puntos)\n",
    "\n",
    "**2.1** (Teoría, 10 pts)  \n",
    "**Explica que hace KMeans, el algoritmo, como encuentra clusters, etc.**\n",
    "\n",
    "Es un algoritmo utilizado para agrupar tus datos (o hacer clústeres) de tal manera que los datos dentro de cada clúster sean lo más parecidos entre sí, y tambien, diferentes a los de los otros clusters. El funcionamiento de este algoritmo se basa en generar aleatoriamente $n$ cantidad de centroides, posteriormente cada dato se asignará al centroide más cercano (medido con la distacia euclidiana) y luego se repetiría tantas veces como sea necesario recalculando los centroides como el promedio de los puntos asignados a cada grupo hasta que los centroides dejen de cambiar y entonces haya convergido el algoritmo.\n",
    "\n",
    "La fórmula con la que se calcula la distancia euclidiana, la cual se necesita para recalcular los centroides es la siguiente:\n",
    "\n",
    "$$\n",
    "\\|x_i - \\mu_k\\|^2 = \\sum_{j=1}^n (x_{ij} - \\mu_{kj})^2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "+ $x_i$: Es el dato en cuestión.\n",
    "+ $\\mu_k$: El centroide.\n",
    "\n",
    "**2.2** (Criterio, 10 pts)  \n",
    "**Explica cómo usarías el método del codo (*elbow method*) y qué limitaciones tiene.**\n",
    "\n",
    "El método del codo consiste en obtener la inercia que mide el modelo para una cantidad $n$ de centroides. Esto se graficaría y según la propuesta del método, el número óptimo de clústeres sería el número en el que en la gráfica se observe un cambio de pendiente importante, lo que formaría una especie de codo.\n",
    "\n",
    "La principal limitación de este método sería que la elección es subjetiva y puede variar de acuerdo con la persona. Con esto me refiero a que la elección nunca sería \"óptima\" pues no se está optimizando nada en esta decisión. Como por ejemplo, en la regresión lineal se utilizan los betas que minimizan el error cuadrático medio del modelo. En cambio en este modelo, nunca se sabe cual es el óptimo dado que nunca se maximiza o minimiza nada, simplemente es a criterio personal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed86a5",
   "metadata": {},
   "source": [
    "## Sección 3: PCA – Análisis de Componentes Principales (20 puntos)\n",
    "\n",
    "**3.1** (Teoría, 5 pts)  \n",
    "**¿Qué significa que la primera componente principal maximiza la varianza?**\n",
    "\n",
    "\n",
    "El Principal Component Analysis es una técnica utilizada para reducir la dimensionalidad de un conjunto de datos, mientras se trata de mantener la mayor cantidad de varianza original. Para realizarlo, primero se comienza con la proyección de los datos sobre un componente:\n",
    "\n",
    "$$\n",
    "\\text{Proy}_{u_1}(x_i) = u_1^T x_i\n",
    "$$\n",
    "\n",
    "De estas proyecciones se calcula la media como:\n",
    "\n",
    "$$\n",
    "u_1^T \\bar{x}\n",
    "$$\n",
    "\n",
    "La varianza de los datos proyectados sobre $u_1$ es:\n",
    "\n",
    "$$\n",
    "\\text{Varianza} = \\frac{1}{N} \\sum_{n=1}^N \\left( u_1^T x_n - u_1^T \\bar{x} \\right)^2\n",
    "$$\n",
    "\n",
    "Realizando las operaciones correspondientes, podemos simplificar a \n",
    "\n",
    "$$\n",
    "\\text{Varianza} = u_1^T S u_1\n",
    "$$\n",
    "\n",
    "* Donde $S$ es la matriz de covarianzas de los datos.\n",
    "\n",
    "A lo que se quiere llegar con esto es proyectar los datos sobre un $u_1$, el cual maximice la varianza capturada, lo que ayudaría a poder explicar de la mejor manera los datos originales en una dimensión menor. Por lo tanto, habría que realizar un proceso de optimización, tal que:\n",
    "\n",
    "$$\n",
    "\\max_{u_1} u_1^T S u_1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{sujeto a:} \\|u_1\\| = 1\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $S$ es la matriz de covarianza de los datos.\n",
    "- $\\|u_1\\| = 1$ restringe que $u_1$ sea un vector de norma 1.\n",
    "\n",
    "Para resolverlo podemos plantear si siguiente lagrangiano:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(u_1, \\lambda) = u_1^T S u_1 - \\lambda (u_1^T u_1 - 1)\n",
    "$$\n",
    "\n",
    "Luego de derivar e igualar a cero, se llega a que:\n",
    "\n",
    "$$\n",
    "S u_1 = \\lambda u_1\n",
    "$$\n",
    "\n",
    "Con este resultado nos podemos dar cuenta de que, lo que estamos resolviendo es un problema de eigenvalores y eigenvectores, pues $u_1$ sería un vector propio de la matriz $S$ de covarianzas y $\\lambda$ un valor propio. \n",
    "\n",
    "Con esto, $u_1$ entonces sería el primer componente principal del PCA y correspondería al vector propio que corresponde al eigenvalor más grande de $S$, En otras palabras la primera componente principal $u_1$ maximiza la varianza de la proyección dado que correspondería al eigenvalor que maximiza $S$ que es la matriz de covarianzas.\n",
    "\n",
    "\n",
    "**3.2** (Cálculo, 10 pts)  \n",
    "**Si tienes 10 variables correlacionadas y aplicas PCA, ¿cuántas componentes necesitas para explicar al menos el 90% de la varianza? Describe cómo se respondería esta pregunta**\n",
    "\n",
    "Esto dependería de los datos y de que tan correlacionados estén entre sí. Pero la resupuesta simple sería que una vez optimizado el problema se tendrían las diferentes $\\lambda_i$, es decir los eigenvalores de la matriz de covarianza. Dadas las restricciones establecidas la suma de estos debería de ser de 1. Entonces, entendiendo los $\\lambda_i$ como el porcentaje de la varianza explicada por el componente $u_i$, se tendrían que usar tantos componentes como $\\lambda_i$ tal que su suma sea de 0.9 o 90% aproximadamente.\n",
    "\n",
    "Por ejemplo, suponiendo que se tienen las siguientes $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\lambda_1 = 0.5, \\quad  \\lambda_2 = 0.3, \\quad \\lambda_3 = 0.1, \\quad \\lambda_4 = 0.05, \\quad ... \\quad \\lambda_{10} = 0.01\n",
    "$$\n",
    "\n",
    "Aquí únicamente habría que usar 3 componentes pues $ \\lambda_1 + \\lambda_2 + \\lambda_3 = 0.5 + 0.3 + 0.1 = 0.9 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23168460",
   "metadata": {},
   "source": [
    "**3.3** (Aplicación, 5 pts)  \n",
    "**¿Tiene sentido usar PCA y usar todas las componentes para predecir? Justifica tu respuesta.**\n",
    "\n",
    "No tendría sentido, pues el objetivo de utilizar el PCA es reducir la dimensionalidad de los datos, entonces solo convendría utilizar la cantidad de componentes que sumen un threshold establecido para la cantidad de varianza explicada que se quiere.\n",
    "\n",
    "Por ejemplo si se tienen 500 variables y con únicamente 10 componentes ya se explica el 90% de la varianza de los datos, pues no sería conveniente utilizar más, dado que esto sería contraproducente al objetivo principal que es reducir la dimensionalidad.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb4a27-9f28-435e-8a52-8a9351c7d47d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Sección 4: Causalidad y Meta-Learners (35 puntos)\n",
    "\n",
    "**4.1** (Conceptual, 10 pts)  \n",
    "**Define el estimando CATE y da un ejemplo de aplicación en negocios.**\n",
    "\n",
    "Para una variable $x$ del conjunto de datos $X$, el CATE se definiría como:\n",
    "\n",
    "$$\n",
    "\\text{CATE}(x) = E[Y(1) - Y(0) \\mid X=x]\n",
    "$$\n",
    "\n",
    "Esto puede interpretarse como el efecto esperado en la predicción dado que una variable $x$ recibió un cambio o un tratamiento. Es decir, que efecto tiene cambiar en la predicción que una cumplir algo vs no cumplirlo.\n",
    "\n",
    "*Ejemplo de aplicación en negocios:*\n",
    "\n",
    "Imaginemos que una tienda lleva 5 años en el mercado vendiendo un producto, y ha notado que cuando ofrecen descuentos a veces ganan más veces que otras veces con sus estrategias, lo que podría deberse a que cantidad de descuento ofrecen. Con el cate entonces podemos medir el efecto esperado que tendría sobre las ventas ofrecer un descuento del 10%, 20%, 30%, etc.\n",
    "\n",
    "Habría que entrenar un modelo, y con el predecir las ventas utilizando los diferentes descuentos que podrían ofrecer. Por ejemplo Dejando fijo el 10%, la predicción es $a$, luego dejando fijo 20% la predicción sería $b$. Con esto calculado habría que calcular el cate y en una gráfica podríamos observar cual sería el descuento óptimo que maximiza las ventas.\n",
    "\n",
    "\n",
    "**4.2** (Comparación, 15 pts)  \n",
    "Explica las diferencias entre los siguientes enfoques para estimar CATE:\n",
    "\n",
    "- **S-Learner**\n",
    "\n",
    "Un S-Learner consiste en entrenar un modelo con los datos originales de entrenamiento. Posteriormente hay que elegir la variable que se quisiera analizar, pues a esta variable se la asignarían multiples valores y el modelo tendría que realizar predicciones sobre estos nuevos valores. Luego de realizar las predicciones se podrá ver como el cambiar una variable puede afectar a la predicción del modelo. \n",
    "\n",
    "Dependiendo de si los datos son discretos o continuos se aplicaría de maneras diferentes:\n",
    "\n",
    "+ Si los datos son continuos se haría lo que se mencionó anteriormente, generar una lista de valores sobre los que se quisiera probar el impacto en la variable a predecir. Posteriormente predecirlo con el modelo, y con ello ver el efecto causal que tiene esa variable midiendo el CATE para cada predicción.\n",
    "\n",
    "+ Si los datos son discretos, entonces, la variable sobre la que se quiere medir el efecto habría que modificarla de tal manera que se puedan tener los datos con un tratamiento o no, es decir, en donde todos sean 1 y todos sean 0. Con esto, nuevamente se realizarían predicciones con ambos grupos de datos y se obtendría el CATE para medir el efecto de aplicar este tratamiento en los datos.\n",
    "\n",
    "Ventajas y desventajas:\n",
    "\n",
    "+ La principal ventaja del S-Learner es que es un modelo muy sencillo, pues incluso la manera de realizarlo llega a ser intuitiva, pues consiste en predecir dejando las variables que se quieren analizar con un valor fijo y al final realizar una resta entre las predicciones para ver si hay diferencias importantes o no las hay.\n",
    "\n",
    "+ La desventaja del S-Learner es que por la forma en la que se entrena el modelo no podría capturar relaciones y efectos tan importantes como si lo podrían hacer un T-Learner o un X-Learner debido a su robustes.\n",
    "\n",
    "\n",
    "- **T-Learner**  \n",
    "\n",
    "En lugar tener un solo modelo para predecir con los datos modificados, ahora se crearían 2 diferentes datasets, uno en donde se tiene el tratamiento y otro donde no se tiene. De igual manera se crearían 2 diferentes modelos, uno entrenado con los datos con tratamiento y otro entrenado con los datos sin tratamiento. Para finalizar se realizan las predicciones a los datos originales del conjunto de prueba con ambos modelos. El objetivo de esto es ver como cambia predecir los mismos datos con 2 modelos diferentes, en los que uno de ellos fue entrenado con tratamiento y el otro no.\n",
    "\n",
    "Nuevamente se calcularían las diferencias entre las predicciones del modelo con tratamiento y las del modelo sin tratamiento. Esta resta sería el CATE del modelo, y nuevamente solo quedaría comprobar si hubo efecto o no. Si las predicciones son similares, entonces significaría que esa variable no tiene efecto al momento de predecir, pues sería casi lo mismo si fuera 1 o 0.\n",
    "\n",
    "Ventajas y desventajas:\n",
    "\n",
    "+ Las ventaja del T-Learner es que entrena modelos separados para los datos con tratamiento y sin tratamiento lo que le da robustes al modelo.\n",
    "\n",
    "+ Una desventaja podría ser que por como está definido podría tender al overfitting.\n",
    "\n",
    "- **X-Learner**  \n",
    "\n",
    "Al igual que los anteriores, este tambien es un modelo para medir el efecto causal. Este modelo solo serviría cuando el tratamiento es para una variable binaria. Para desarrollarlo primero habría que crear 2 modelos entrenados con y sin tratamiento al igual que en el T-Learner. Con estos modelos ahora hay que calcular los efectos individuales contrafactuales $D_0$ y $D_1$. Posteriormente se crean 2 modelos nuevos $\\tau_{0} (x) $ y $\\tau_{1} (x) $, los cuales se usarían para predecir $D_0$ y $D_1$.\n",
    "\n",
    "Posteriormente se calcula el propensity score, el cual servirá para calcular el CATE final, pues este propensity score es la probabilidad que tendría una observación de recibir un tratamiento condicional. Entonces el CATE al final sería un promedio ponderado por el propensity score de los CATES estimados con $\\tau_{0} (x) $ y $\\tau_{1} (x) $, el cual le daría más peso a al modelo opuesto al cual pertenecería.\n",
    "\n",
    "Ventajas y Desventajas:\n",
    "\n",
    "\n",
    "+ La principal ventaja del X-Learner es la combinación de los otros 2 modelos para proporcionar robustés\n",
    "\n",
    "+ Una desventaja podría ser que el modelo llega a ser un poco complicado, e incluso costoso computacionalmente debido a que se tienen que entrenar muchos modelos.\n",
    "\n",
    "\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\hat{e}(x)$ sería el propensity score para cada observación, es decir, la probabilidad de recibir tratamiento.\n",
    "\n",
    "\n",
    "**Incluye ventajas y desventajas.**\n",
    "\n",
    "**4.3** (Implementación, 10 pts)  \n",
    "Te entregan un dataset con una columna `treatment` (0/1), un `outcome`, y varias covariables. ¿Cómo entrenarías un X-Learner paso a paso?\n",
    "\n",
    "1. Se inicializan 2 modelos diferentes, los cuales se entrenan con 2 datasets diferentes, uno en el que se tiene el tratamiento y o otro en el que no se tiene, igual a como se hace en el T-Learner. Con estos modelos podemos calcular  $D_0$ y $D_1$, los cuales se pueden definir de la siguiente manera:\n",
    "\n",
    "    + $D_0$ sería lo que pasó menos lo que pasaría menos lo que hubiera pasado sin tratamiento, entonces:\n",
    "\n",
    "$$\n",
    "D^0_i = Y_i - M_0(X_i)\n",
    "$$\n",
    "    \n",
    "* $D_1$ sería lo que hubiera pasado con tratamiento menos lo que pasó, entonces:\n",
    "\n",
    "$$\n",
    "D^1_i = M_1(X_i) - Y_i \n",
    "$$\n",
    "\n",
    "2. Con $D_0$ y $D_1$ calculados ahora se inicializan 2 modelos nuevos:\n",
    "\n",
    "    + $\\tau_{0} (x) $ sería un modelo entrado con los datos con tratamiento aplicado y trataría de predecir los efectos $D0$.\n",
    "\n",
    "    + $\\tau_{1} (x) $ sería un modelo entrado con los datos sin tratamiento y se trataría de predecir los efectos $D1$.\n",
    "\n",
    "3. Con todo lo anterior se calcularía el propensity score, el cual es la probabilidad que tendría una observación de recibir un tratamiento condicional. Para calcularlo, nuevamente se inicializaría un nuevo modelo, el cual se entrenaría utilizando los datos de prueba sin la variable que se está analizando, y con ello tratar de predecir la probabilidad de que se reciba tratamiento.\n",
    "\n",
    "4. Para finalizar, habría que calcular ahora el CATE. Como se mencionó anteriormente, se utilizaría  $\\tau_{0} (x) $ y $\\tau_{1} (x) $  para encontrar los CATEs, por lo que se tendrían 2 cates para una observación, entonces con ello se calcularía el CATE con un promedio ponderado por el propensity score, el cual le daría más peso a al modelo opuesto al cual pertenecería. La forma de calcularlo es:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}(x) = (1 - \\hat{e}(x)) \\cdot \\hat{\\tau}_0(x) + \\hat{e}(x) \\cdot \\hat{\\tau}_1(x)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\hat{e}(x)$ sería el propensity score para cada observación, es decir, la probabilidad de recibir tratamiento.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
