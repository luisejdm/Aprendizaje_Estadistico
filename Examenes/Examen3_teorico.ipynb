{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee7a401",
   "metadata": {},
   "source": [
    "### Luis Eduardo Jiménez del Muro - 24/04/2025\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1495cb61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Examen – Modelos de Ensamble: Árboles y Boosting\n",
    "\n",
    "**Instrucciones**: Justifica cada respuesta de manera clara. Usa fórmulas donde aplique y sé específico en tus argumentos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf2734",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Sección I: Árboles de decisión (15 puntos)\n",
    "\n",
    "### 1. (5 pts)  \n",
    "**Explica cómo se construye un árbol y qué criterio usa para decidir los splits. Explica tanto para el caso de clasificación como de regresión.**\n",
    "\n",
    "Se construye de tal manera donde gracias a los niveles y nodos clasificas a tus datos dependiendo de si sobrepasan o no un umbral establecido. El úmbral óptimo es donde se realiza el split, y para obtenerlo se realiza de la siguiente manera:\n",
    "\n",
    "\n",
    "#### Regresión\n",
    "\n",
    "Primero el algoritmo utiliza todas las características y umbrales de división posibles, y con ello, seleccionar los umbrales que logren la mayor reducción de varianza.\n",
    "\n",
    "$$\n",
    "\\text{Reducción de varianza} = \\text{Varianza total} - \\left( \\frac{N_1}{N} \\times \\text{Var}(G1) + \\frac{N_2}{N} \\times \\text{Var}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Esta formula compara la varianza total de las ramas con la varianza despues de ser ponderada por la cantidad de datos en cada rama.\n",
    "\n",
    "#### Clasificación\n",
    "\n",
    "Igual que el de regresión, el algoritmo utiliza todas las características y umbrales de división posibles, y con ello, seleccionar que los umbrales que, ahora en este caso, logren la mayor reducción de impureza, calculada por medio de Gini o la Entropía.\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum p_i^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "Entropía = -\\sum p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Ganancia de Impureza} = \\text{Impureza Nodo Padre} - \\left( \\frac{N_1}{N} \\times \\text{Impureza}(G1) + \\frac{N_2}{N} \\times \\text{Impureza}(G2) \\right)\n",
    "$$\n",
    "\n",
    "Esta formula compara la impureza del nodo padre con la impureza ponderada por la cantidad de datos en cada rama.\n",
    "\n",
    "\n",
    "### 2. (5 pts)  \n",
    "**Da un ejemplo de sobreajuste en un árbol de decisión. Explica cómo se podría evitar sin necesidad de usar ensambles.**\n",
    "\n",
    "Un arbol puede presentar overfitting si es que tenemos pocos datos, y el arbol que creamos es de mucha profundidad. Esto haría que las reglas establecidas en nuestro árbol de decisión sean tan específicas hasta el punto en el que llegarían a memorizar nuestros datos. La forma de evitar esto es controlar la profundidad máxima de nuestro árbol, para que si no tenemos muchos datos, se limite más y no llegue a memorizar.\n",
    "\n",
    "### 3. (5 pts)  \n",
    "**Si te fijas, en clase nunca hicimos escalamiento (`StandardScaler`). ¿Por qué los árboles no lo necesitan?**\n",
    "\n",
    "Porque el funcionamiento de los árboles se basa en simples reglas donde hay que comprobar si pasa o no pasa el umbral, por lo tanto, el escalamiento no es necesario. En cambio, modelos como el knn donde medimos la distancia entre los vecinos, ahí si es importante que los datos estén escalados y no creemos un mal modelo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c5e26",
   "metadata": {},
   "source": [
    "\n",
    "## Sección II: Random Forest (20 puntos)\n",
    "\n",
    "### 4. (10 pts)  \n",
    "**Explica cómo funciona un Random Forest. ¿En qué se basa? ¿Por qué es una buena idea?**\n",
    "\n",
    "Utilizando el método de bootstrap, se realiza un muestreo aleatorio de nuestros datos originales y con ello crear varios árboles diferentes. Estos árboles son entrenados y para realizar las predicciones cambiará dependiendo de si es regresión o clasificación. \n",
    "\n",
    "+ Si es regresión se promediará la predicción de todos los árboles creados.\n",
    "+ Si es de clasificación, se toma la categoría más repetida.\n",
    "\n",
    "El modelo de Random Forest es buena idea porque al crear tantos árboles distintos automáticamente estamos reduciendo las probabilidades de hacer overfitting, así como tambien mejorar las predicciones realizando promedios de muchos modelos diferentes, los cuales son entrenados con datos diferentes gracias al bootstrap.\n",
    "\n",
    "### 5. (10 pts)  \n",
    "**Menciona dos ventajas y dos desventajas del Random Forest comparado con un solo árbol. Sé específico, no generalices.**\n",
    "\n",
    "#### Ventajas\n",
    "\n",
    "+ Mayor poder predictivo de comparado a los arboles de decisión normales porque al combinar varios árboles diferentes se puede lograr entrenar un modelo que capte relaciones más complejas en nuestros datos.\n",
    "+ Gracias al muestreo aleatorio se reduce en gran medida el overfitting, dado que el arbol normal puede tender a ajustarse mucho a los datos originales.\n",
    "\n",
    "#### Desventajas\n",
    "\n",
    "+ Dado que se generan muchos árboles diferentes el modelo pierde interpretabilidad.\n",
    "+ Es mas costoso computacionalmente al tener que generar y entrenar muchos árboles.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f75794d",
   "metadata": {},
   "source": [
    "## Sección III: Gradient Boosting (25 puntos)\n",
    "\n",
    "### 6. (10 pts)  \n",
    "**Explica, paso a paso, cómo funciona el algoritmo de **Gradient Boosting**. Incluye el concepto de residuales y cómo se minimiza la pérdida en cada iteración.**\n",
    "\n",
    "Es un modelo en el que se van construyendo árboles de decisión de manera iterativa, donde el modelo aprende con los residuales del árbol anterior, y con ello, se actualiza haciendo un modelo para equivocarse menos. Para minimizar la pérdida en cada iteración, cada nuevo arbol se ajusta a los residuales del arbol anterior. Al igual que en el descenso en gradiente, para actualizar el los árboles con cada iteración se utiliza el factor $\\alpha$ de learning rate y se repite hasta converger.\n",
    "\n",
    "La predicción del modelo sería de la siguiente manera:\n",
    "\n",
    "$$F_m(x) = F_{m - 1}(x) + \\nu (\\text{nuevo árbol})$$\n",
    "\n",
    "La predicción del árbol anterior será el promedio de los residuales de cada hoja.\n",
    "\n",
    "En clasificación, la predicción estaría en términos de log-odds.\n",
    "\n",
    "\n",
    "### 7. (15 pts)  \n",
    "**¿Cuál es la diferencia entre Gradient Boosting y Random Forest en términos de cómo combinan los árboles?**\n",
    "\n",
    "+ En Random Forest simplemente se crean muchos árboles independientes con datos diferentes gracias a los muestreos aleatorios del bootstrap. Su predicción será el promedio de las predicciones de todos los árboles.\n",
    "\n",
    "+ Mientras tanto en Gradient boosting, la forma de combinar los árboles no es simultantea, sino que se hace de uno por uno y la predicción del árbol anterior sería promediar los residuales de cada hoja, sumándole los del nuevo arbol ponderándolos por un factor $\\alpha$ de learning rate.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323cad6-0afa-4c28-9ce0-07c970efdb6b",
   "metadata": {},
   "source": [
    "## Sección IV: XGBoost (20 puntos)\n",
    "\n",
    "### 8. (10 pts)  \n",
    "**Explica cómo XGBoost optimiza el proceso de boosting usando una expansión de Taylor de segundo orden.**\n",
    "\n",
    "Este modelo es la versión optimizada del Gradient Boosting. Este es un modelo aditivo de árboles, en otras palabras, el modelo final se construye sumando árboles, de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\hat{y}_i^{(t)}$ es la predicción del ejemplo $i$ en la iteración $t$\n",
    "- $f_t(x_i)$ es el nuevo árbol que se entrena en la iteración $t$\n",
    "\n",
    "\n",
    "Con cada iteración, se busca que el nuervo árbol $f_t$ minimice la contribución marginal a la función de pérdida total además se utiliza un factor $\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2$ que es una penalización por la complejidad del árbol $f_k$.\n",
    "\n",
    "\n",
    "Dado que esta función de pérdada es muy compleja, se utiliza la expansión de Taylor para aproximar esta función por medio de polinomios, y de esta manera hacerla más facil de optimizar:\n",
    "\n",
    "$$\n",
    "f(x) \\approx f(a) + f'(a)(x - a) + \\frac{1}{2} f''(a)(x - a)^2\n",
    "$$\n",
    "\n",
    "Despues de áplicar la expansión de Taylor en XGBoost y utilizando el gradiente y hessiano, se llega, a que la función que se optimiza en cada iteración es:\n",
    "\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}^{(t)} = \\sum_{i=1}^n \\left[ g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2 \\right] + \\Omega(f_t)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Cada nuevo árbol $f_t$ asigna un valor constante $w_j$, entonces, sustituyendo esto en la antigua fución objetivo y escribiendo $\\Omega(f)$ explícitamente:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "\\sum_{i \\in R_j} g_i w_j + \\frac{1}{2} \\sum_{i \\in R_j} h_i w_j^2\n",
    "\\right] + \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
    "$$\n",
    "\n",
    "Simplificando:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T} \\left[\n",
    "G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2\n",
    "\\right] + \\gamma T\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$G_j = \\sum_{i \\in R_j} g_i$\n",
    "\n",
    "$H_j = \\sum_{i \\in R_j} h_i$\n",
    "\n",
    "\n",
    "### 9. (5 pts)  \n",
    "**¿Qué es el *similarity score*? ¿Qué es el *output value*? ¿De dónde salen estas fórmulas y cuál es su interpretación?**\n",
    "\n",
    "El output value es el valor predicho en cada hoja y la fórmula viene de minimizar la pérdida en cada hoja. Derivando con respecto a $w_j$, llegamos al siguiente output value:\n",
    "\n",
    "$$\n",
    "\\textbf{Output value} = w_j^* = -\\frac{G_j}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "El similarity score es una medida para evaluar cuanto mejora la pérdida por haber utilizado el óptimo. La formula viene de sustituir el output value óptimo $w_j^*$ en la función objetivo. La función de objetivo o de pérdida por hoja es:\n",
    "\n",
    "$$ \\mathcal{L}_j(w_j) = G_j w_j + \\frac{1}{2} (H_j + \\lambda) w_j^2 $$  \n",
    "\n",
    "Sustituyendo el output value óptimo $w_j^*$ en la función objetivo:\n",
    "\n",
    "$$\n",
    "\\text{Similarity Score} = \\frac{G_j^2}{H_j + \\lambda}\n",
    "$$\n",
    "\n",
    "Posteriormente, con el similarity score podemos calcular el Gain, lo que nos ayudará a ver si es conveniente realizar un nuevo split o aumentar el grado de complejidad de nuestro modelo.\n",
    "\n",
    "#### Interpretación\n",
    "\n",
    "+ El output value lo podemos ver como el ajuste óptimo que minimiza la pérdida en el nodo.\n",
    "+ El similarity nos indica si nuestra división es buena, para que posteriormente con el gain podamos ver si aumentamos o no la complejidad del modelo. \n",
    "\n",
    "\n",
    "### 10. (5 pts)  \n",
    "**XGBoost y otros modelos de gradient boosting permiten evaluar la importancia de las variables con diferentes métricas: `weight` y `gain`. Explica qué representa cada una. ¿Cuál crees que es más útil para interpretar un modelo y por qué?**\n",
    "\n",
    "+ *Weight*: Veces que se usa una variable en el modelo.\n",
    "+ *Gain*: En promedio cuanto baja la función de pérdida una variable.\n",
    "\n",
    "En mi opinión la más importante para interpretar un modelo es el gain, porque ahí podemos ver cual es la variable que mejor ajusta nuestro modelo minimizando la pérdida. Mientras que weight aunque tambien es importante, puede ser que una variable que se use mucho no necesariamente sea buena para lograr que el modelo tenga un mejor ajuste.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a417589b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Sección V: LightGBM y CatBoost (15 puntos)\n",
    "\n",
    "### 10. (5 pts)  \n",
    "**Explica qué es **histogram-based splitting** y cómo lo implementa LightGBM para ganar velocidad.**\n",
    "\n",
    "Es un proceso que usa LightGBM para que el modelo pueda ser más rápido que XGBoost, dado que, lo que hace es dividir nuestro dataset en una cantidad de bins determinada, por lo que, al agrupar de esta manera nuestros datos se reducen mucho la cantidad de posibles divisiones en el árbol y se vuelve más rápido.\n",
    "\n",
    "### 11. (5 pts)  \n",
    "**¿Qué problema específico resuelve CatBoost respecto al manejo de variables categóricas? ¿Cómo lo hace?**\n",
    "\n",
    "Lo que hace catboost es que para entrenar el modelo no es necesario realizar encoding manual en las variables de nuestro dataset, sino que el modelo ya lo tiene de forma nativa. La forma en la que CatBoost maneja las variables categóricas funciona de la siguiente manera:\n",
    "\n",
    "1. Ordena las filas de forma aleatoria.\n",
    "2. Para cada fila se codifica la categoría con el promedio del target acumulato hasta ese momento con las observaciones anteriores.\n",
    "\n",
    "Dado que nunca se incluye el target de la fila actual, sino de la enteror, esto no introduce data leakege en el modelo.\n",
    "\n",
    "Esto es realizado con varias permutaciones del dataset, para al final promediar los resultados y lograr un encoding estable.\n",
    "\n",
    "### 12. (5 pts)  \n",
    "**Compara LightGBM y CatBoost: ¿cuándo usarías uno sobre el otro? Sé claro y justifica en base a tipo de datos, velocidad o precisión.**\n",
    "\n",
    "+ LightGBM se usaría en un caso donde se quiere un modelo robusto en muy poco tiempo, en dónde haya muy pocas o ninguna variable categórica, mientras que Catboost lo usaría cuando quiera un modelo rápido tambien pero donde tenga muchas variables categóricas en los datos.\n",
    "\n",
    "+ Además LightGBM es muy bueno para cuando se tienen enormes cantidades de datos gracias a su división en Bins, por lo que si tenemos muchos datos no sería tan buena idea usar Catboost porque podría tardar mucho más tiempo en entrenar.\n",
    "\n",
    "+ Por otro lado, si son pocos datos, LightGBM podría ser menos preciso en comparación a Catboost.\n",
    "\n",
    "\n",
    "## Sección VI: Power Analysis (5 puntos)\n",
    "\n",
    "### 13. (5 pts)  \n",
    "**¿Qué es un *power analysis* y para qué sirve? ¿En qué contexto lo hemos usado en clase y por qué es importante antes de correr un experimento?**\n",
    "\n",
    "Es una técnica utilizada para determinar el tamaño de muestra necesario para lograr observar un efecto con una probabilidad determinada, así como tambie, para ver que tan probable es que mi estudio detecte un efecto real (si es que existe). Por ejemplo, en clase lo hemos utilizado para calcular el tamaño de muestra necesario en los AB testing. Es importante antes de realizar los experimentos para que antes de realizarlos sepamos cuantos datos recopilar para poder lograr resultados que estadísticamente demuestren lo que queremo probar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
